{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\seung\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\seung\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\seung\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as numpy\n",
    "import nltk\n",
    "nltk.download(['punkt','averaged_perceptron_tagger','wordnet'])\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import contractions\n",
    "import langdetect\n",
    "from langdetect import detect \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open file\n",
    "with open('main/data/youtube_comments.csv',encoding=\"utf8\") as file:\n",
    "    df = pd.read_csv(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>link_title</th>\n",
       "      <th>channel</th>\n",
       "      <th>no_of_views</th>\n",
       "      <th>time_uploaded</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>comment_posted</th>\n",
       "      <th>no_of_replies</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>downvotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.youtube.com/watch?v=UXSqcCYMFE4</td>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>I swear, whoever is playing Plague Inc needs t...</td>\n",
       "      <td>Leo Hurtt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>486</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.youtube.com/watch?v=UXSqcCYMFE4</td>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>I am getting real tired of being part of  majo...</td>\n",
       "      <td>Sergio Villasenor</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>426</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.youtube.com/watch?v=UXSqcCYMFE4</td>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>This channel's COVID reports are a model of ho...</td>\n",
       "      <td>Eyerleth</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>797</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.youtube.com/watch?v=UXSqcCYMFE4</td>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>To people who understand science, the developm...</td>\n",
       "      <td>gowzahr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.youtube.com/watch?v=UXSqcCYMFE4</td>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>So with these new variants this whole thing fe...</td>\n",
       "      <td>Itsonlyafleshwound</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>205</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           url  \\\n",
       "0  https://www.youtube.com/watch?v=UXSqcCYMFE4   \n",
       "1  https://www.youtube.com/watch?v=UXSqcCYMFE4   \n",
       "2  https://www.youtube.com/watch?v=UXSqcCYMFE4   \n",
       "3  https://www.youtube.com/watch?v=UXSqcCYMFE4   \n",
       "4  https://www.youtube.com/watch?v=UXSqcCYMFE4   \n",
       "\n",
       "                                          link_title  channel   no_of_views  \\\n",
       "0  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "1  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "2  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "3  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "4  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "\n",
       "  time_uploaded                                            comment  \\\n",
       "0  Jan 30, 2021  I swear, whoever is playing Plague Inc needs t...   \n",
       "1  Jan 30, 2021  I am getting real tired of being part of  majo...   \n",
       "2  Jan 30, 2021  This channel's COVID reports are a model of ho...   \n",
       "3  Jan 30, 2021  To people who understand science, the developm...   \n",
       "4  Jan 30, 2021  So with these new variants this whole thing fe...   \n",
       "\n",
       "               author  comment_posted  no_of_replies upvotes  downvotes  \n",
       "0           Leo Hurtt             NaN            NaN     486        NaN  \n",
       "1   Sergio Villasenor             NaN            NaN     426        NaN  \n",
       "2            Eyerleth             NaN            NaN     797        NaN  \n",
       "3             gowzahr             NaN            NaN     105        NaN  \n",
       "4  Itsonlyafleshwound             NaN            NaN     205        NaN  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check head\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "df.drop(['comment_posted','no_of_replies','downvotes','url','author'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link_title 0\n",
      "channel 0\n",
      "no_of_views 0\n",
      "time_uploaded 0\n",
      "comment 0\n",
      "upvotes 145\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "for col in df.columns:\n",
    "    print(col, df[col].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill null values for upvotes ( = 0 likes) as 0\n",
    "df[\"upvotes\"].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link_title 0\n",
      "channel 0\n",
      "no_of_views 0\n",
      "time_uploaded 0\n",
      "comment 0\n",
      "upvotes 0\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "for col in df.columns:\n",
    "    print(col, df[col].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_title</th>\n",
       "      <th>channel</th>\n",
       "      <th>no_of_views</th>\n",
       "      <th>time_uploaded</th>\n",
       "      <th>comment</th>\n",
       "      <th>upvotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>I swear, whoever is playing Plague Inc needs t...</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>I am getting real tired of being part of  majo...</td>\n",
       "      <td>426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>This channel's COVID reports are a model of ho...</td>\n",
       "      <td>797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>To people who understand science, the developm...</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>So with these new variants this whole thing fe...</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          link_title  channel   no_of_views  \\\n",
       "0  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "1  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "2  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "3  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "4  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "\n",
       "  time_uploaded                                            comment upvotes  \n",
       "0  Jan 30, 2021  I swear, whoever is playing Plague Inc needs t...     486  \n",
       "1  Jan 30, 2021  I am getting real tired of being part of  majo...     426  \n",
       "2  Jan 30, 2021  This channel's COVID reports are a model of ho...     797  \n",
       "3  Jan 30, 2021  To people who understand science, the developm...     105  \n",
       "4  Jan 30, 2021  So with these new variants this whole thing fe...     205  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "link_title       We Know More About Those COVID-19 Variants. It...\n",
       "channel                                                    SciShow\n",
       "no_of_views                                           85,566 views\n",
       "time_uploaded                                         Jan 30, 2021\n",
       "comment          If there's anything good from this, it's that ...\n",
       "upvotes                                                          9\n",
       "no_contract      [If, there is, anything, good, from, this,, it...\n",
       "Name: 10, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### expanding contractions = don't --> do not ###\n",
    "\n",
    "# add a column with contractions removed\n",
    "df['no_contract'] = df['comment'].apply(lambda x: [contractions.fix(word) for word in x.split() ])\n",
    "df.loc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_title</th>\n",
       "      <th>channel</th>\n",
       "      <th>no_of_views</th>\n",
       "      <th>time_uploaded</th>\n",
       "      <th>comment</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>no_contract</th>\n",
       "      <th>comments_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>I swear, whoever is playing Plague Inc needs t...</td>\n",
       "      <td>486</td>\n",
       "      <td>[I, swear,, whoever, is, playing, Plague, Inc,...</td>\n",
       "      <td>I swear, whoever is playing Plague Inc needs t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>I am getting real tired of being part of  majo...</td>\n",
       "      <td>426</td>\n",
       "      <td>[I, am, getting, real, tired, of, being, part,...</td>\n",
       "      <td>I am getting real tired of being part of major...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>This channel's COVID reports are a model of ho...</td>\n",
       "      <td>797</td>\n",
       "      <td>[This, channel's, COVID, reports, are, a, mode...</td>\n",
       "      <td>This channel's COVID reports are a model of ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>To people who understand science, the developm...</td>\n",
       "      <td>105</td>\n",
       "      <td>[To, people, who, understand, science,, the, d...</td>\n",
       "      <td>To people who understand science, the developm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>So with these new variants this whole thing fe...</td>\n",
       "      <td>205</td>\n",
       "      <td>[So, with, these, new, variants, this, whole, ...</td>\n",
       "      <td>So with these new variants this whole thing fe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          link_title  channel   no_of_views  \\\n",
       "0  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "1  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "2  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "3  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "4  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "\n",
       "  time_uploaded                                            comment upvotes  \\\n",
       "0  Jan 30, 2021  I swear, whoever is playing Plague Inc needs t...     486   \n",
       "1  Jan 30, 2021  I am getting real tired of being part of  majo...     426   \n",
       "2  Jan 30, 2021  This channel's COVID reports are a model of ho...     797   \n",
       "3  Jan 30, 2021  To people who understand science, the developm...     105   \n",
       "4  Jan 30, 2021  So with these new variants this whole thing fe...     205   \n",
       "\n",
       "                                         no_contract  \\\n",
       "0  [I, swear,, whoever, is, playing, Plague, Inc,...   \n",
       "1  [I, am, getting, real, tired, of, being, part,...   \n",
       "2  [This, channel's, COVID, reports, are, a, mode...   \n",
       "3  [To, people, who, understand, science,, the, d...   \n",
       "4  [So, with, these, new, variants, this, whole, ...   \n",
       "\n",
       "                                        comments_str  \n",
       "0  I swear, whoever is playing Plague Inc needs t...  \n",
       "1  I am getting real tired of being part of major...  \n",
       "2  This channel's COVID reports are a model of ho...  \n",
       "3  To people who understand science, the developm...  \n",
       "4  So with these new variants this whole thing fe...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# though sentences are tokenized, the contractions are tokenized into one token\n",
    "# i.e. We've --> [\"we have\"] instead of [\"we\", \"have\"]\n",
    "# we'll join the tokens back into singular strings so we can tokenize them later\n",
    "\n",
    "# map(str,l) takes the iterable l (all values in df['no_contract']) and stringifies it w/ str\n",
    "df['comments_str'] = [' '.join(map(str, l)) for l in df['no_contract']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_title</th>\n",
       "      <th>channel</th>\n",
       "      <th>no_of_views</th>\n",
       "      <th>time_uploaded</th>\n",
       "      <th>comment</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>no_contract</th>\n",
       "      <th>comments_str</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lower</th>\n",
       "      <th>no_punct</th>\n",
       "      <th>no_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>I swear, whoever is playing Plague Inc needs t...</td>\n",
       "      <td>486</td>\n",
       "      <td>[I, swear,, whoever, is, playing, Plague, Inc,...</td>\n",
       "      <td>I swear, whoever is playing Plague Inc needs t...</td>\n",
       "      <td>[I, swear, ,, whoever, is, playing, Plague, In...</td>\n",
       "      <td>[i, swear, ,, whoever, is, playing, plague, in...</td>\n",
       "      <td>[i, swear, whoever, is, playing, plague, inc, ...</td>\n",
       "      <td>[swear, whoever, playing, plague, inc, needs, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>I am getting real tired of being part of  majo...</td>\n",
       "      <td>426</td>\n",
       "      <td>[I, am, getting, real, tired, of, being, part,...</td>\n",
       "      <td>I am getting real tired of being part of major...</td>\n",
       "      <td>[I, am, getting, real, tired, of, being, part,...</td>\n",
       "      <td>[i, am, getting, real, tired, of, being, part,...</td>\n",
       "      <td>[i, am, getting, real, tired, of, being, part,...</td>\n",
       "      <td>[getting, real, tired, part, major, historical...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>This channel's COVID reports are a model of ho...</td>\n",
       "      <td>797</td>\n",
       "      <td>[This, channel's, COVID, reports, are, a, mode...</td>\n",
       "      <td>This channel's COVID reports are a model of ho...</td>\n",
       "      <td>[This, channel, 's, COVID, reports, are, a, mo...</td>\n",
       "      <td>[this, channel, 's, covid, reports, are, a, mo...</td>\n",
       "      <td>[this, channel, 's, covid, reports, are, a, mo...</td>\n",
       "      <td>[channel, 's, covid, reports, model, science, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>To people who understand science, the developm...</td>\n",
       "      <td>105</td>\n",
       "      <td>[To, people, who, understand, science,, the, d...</td>\n",
       "      <td>To people who understand science, the developm...</td>\n",
       "      <td>[To, people, who, understand, science, ,, the,...</td>\n",
       "      <td>[to, people, who, understand, science, ,, the,...</td>\n",
       "      <td>[to, people, who, understand, science, the, de...</td>\n",
       "      <td>[people, understand, science, development, vac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>So with these new variants this whole thing fe...</td>\n",
       "      <td>205</td>\n",
       "      <td>[So, with, these, new, variants, this, whole, ...</td>\n",
       "      <td>So with these new variants this whole thing fe...</td>\n",
       "      <td>[So, with, these, new, variants, this, whole, ...</td>\n",
       "      <td>[so, with, these, new, variants, this, whole, ...</td>\n",
       "      <td>[so, with, these, new, variants, this, whole, ...</td>\n",
       "      <td>[new, variants, whole, thing, feels, like, fou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          link_title  channel   no_of_views  \\\n",
       "0  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "1  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "2  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "3  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "4  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "\n",
       "  time_uploaded                                            comment upvotes  \\\n",
       "0  Jan 30, 2021  I swear, whoever is playing Plague Inc needs t...     486   \n",
       "1  Jan 30, 2021  I am getting real tired of being part of  majo...     426   \n",
       "2  Jan 30, 2021  This channel's COVID reports are a model of ho...     797   \n",
       "3  Jan 30, 2021  To people who understand science, the developm...     105   \n",
       "4  Jan 30, 2021  So with these new variants this whole thing fe...     205   \n",
       "\n",
       "                                         no_contract  \\\n",
       "0  [I, swear,, whoever, is, playing, Plague, Inc,...   \n",
       "1  [I, am, getting, real, tired, of, being, part,...   \n",
       "2  [This, channel's, COVID, reports, are, a, mode...   \n",
       "3  [To, people, who, understand, science,, the, d...   \n",
       "4  [So, with, these, new, variants, this, whole, ...   \n",
       "\n",
       "                                        comments_str  \\\n",
       "0  I swear, whoever is playing Plague Inc needs t...   \n",
       "1  I am getting real tired of being part of major...   \n",
       "2  This channel's COVID reports are a model of ho...   \n",
       "3  To people who understand science, the developm...   \n",
       "4  So with these new variants this whole thing fe...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [I, swear, ,, whoever, is, playing, Plague, In...   \n",
       "1  [I, am, getting, real, tired, of, being, part,...   \n",
       "2  [This, channel, 's, COVID, reports, are, a, mo...   \n",
       "3  [To, people, who, understand, science, ,, the,...   \n",
       "4  [So, with, these, new, variants, this, whole, ...   \n",
       "\n",
       "                                               lower  \\\n",
       "0  [i, swear, ,, whoever, is, playing, plague, in...   \n",
       "1  [i, am, getting, real, tired, of, being, part,...   \n",
       "2  [this, channel, 's, covid, reports, are, a, mo...   \n",
       "3  [to, people, who, understand, science, ,, the,...   \n",
       "4  [so, with, these, new, variants, this, whole, ...   \n",
       "\n",
       "                                            no_punct  \\\n",
       "0  [i, swear, whoever, is, playing, plague, inc, ...   \n",
       "1  [i, am, getting, real, tired, of, being, part,...   \n",
       "2  [this, channel, 's, covid, reports, are, a, mo...   \n",
       "3  [to, people, who, understand, science, the, de...   \n",
       "4  [so, with, these, new, variants, this, whole, ...   \n",
       "\n",
       "                                        no_stopwords  \n",
       "0  [swear, whoever, playing, plague, inc, needs, ...  \n",
       "1  [getting, real, tired, part, major, historical...  \n",
       "2  [channel, 's, covid, reports, model, science, ...  \n",
       "3  [people, understand, science, development, vac...  \n",
       "4  [new, variants, whole, thing, feels, like, fou...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenize the comments using NLTK\n",
    "df['tokenized'] = df['comments_str'].apply(word_tokenize)\n",
    "df.head()\n",
    "\n",
    "# convert characters to lowercase\n",
    "df['lower'] = df['tokenized'].apply(lambda x: [word.lower() for word in x])\n",
    "\n",
    "# remove punctuations\n",
    "punct = string.punctuation\n",
    "# make a list only if token is not a punctuation\n",
    "df['no_punct'] = df['lower'].apply(lambda x: [word for word in x if word not in punct])\n",
    "\n",
    "# remove stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "df['no_stopwords'] = df['no_punct'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_title</th>\n",
       "      <th>channel</th>\n",
       "      <th>no_of_views</th>\n",
       "      <th>time_uploaded</th>\n",
       "      <th>comment</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>no_contract</th>\n",
       "      <th>comments_str</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lower</th>\n",
       "      <th>no_punct</th>\n",
       "      <th>no_stopwords</th>\n",
       "      <th>pos_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>I swear, whoever is playing Plague Inc needs t...</td>\n",
       "      <td>486</td>\n",
       "      <td>[I, swear,, whoever, is, playing, Plague, Inc,...</td>\n",
       "      <td>I swear, whoever is playing Plague Inc needs t...</td>\n",
       "      <td>[I, swear, ,, whoever, is, playing, Plague, In...</td>\n",
       "      <td>[i, swear, ,, whoever, is, playing, plague, in...</td>\n",
       "      <td>[i, swear, whoever, is, playing, plague, inc, ...</td>\n",
       "      <td>[swear, whoever, playing, plague, inc, needs, ...</td>\n",
       "      <td>[(swear, JJ), (whoever, WP), (playing, VBG), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>I am getting real tired of being part of  majo...</td>\n",
       "      <td>426</td>\n",
       "      <td>[I, am, getting, real, tired, of, being, part,...</td>\n",
       "      <td>I am getting real tired of being part of major...</td>\n",
       "      <td>[I, am, getting, real, tired, of, being, part,...</td>\n",
       "      <td>[i, am, getting, real, tired, of, being, part,...</td>\n",
       "      <td>[i, am, getting, real, tired, of, being, part,...</td>\n",
       "      <td>[getting, real, tired, part, major, historical...</td>\n",
       "      <td>[(getting, VBG), (real, JJ), (tired, JJ), (par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>This channel's COVID reports are a model of ho...</td>\n",
       "      <td>797</td>\n",
       "      <td>[This, channel's, COVID, reports, are, a, mode...</td>\n",
       "      <td>This channel's COVID reports are a model of ho...</td>\n",
       "      <td>[This, channel, 's, COVID, reports, are, a, mo...</td>\n",
       "      <td>[this, channel, 's, covid, reports, are, a, mo...</td>\n",
       "      <td>[this, channel, 's, covid, reports, are, a, mo...</td>\n",
       "      <td>[channel, 's, covid, reports, model, science, ...</td>\n",
       "      <td>[(channel, NN), ('s, POS), (covid, NN), (repor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>To people who understand science, the developm...</td>\n",
       "      <td>105</td>\n",
       "      <td>[To, people, who, understand, science,, the, d...</td>\n",
       "      <td>To people who understand science, the developm...</td>\n",
       "      <td>[To, people, who, understand, science, ,, the,...</td>\n",
       "      <td>[to, people, who, understand, science, ,, the,...</td>\n",
       "      <td>[to, people, who, understand, science, the, de...</td>\n",
       "      <td>[people, understand, science, development, vac...</td>\n",
       "      <td>[(people, NNS), (understand, VBP), (science, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>So with these new variants this whole thing fe...</td>\n",
       "      <td>205</td>\n",
       "      <td>[So, with, these, new, variants, this, whole, ...</td>\n",
       "      <td>So with these new variants this whole thing fe...</td>\n",
       "      <td>[So, with, these, new, variants, this, whole, ...</td>\n",
       "      <td>[so, with, these, new, variants, this, whole, ...</td>\n",
       "      <td>[so, with, these, new, variants, this, whole, ...</td>\n",
       "      <td>[new, variants, whole, thing, feels, like, fou...</td>\n",
       "      <td>[(new, JJ), (variants, NNS), (whole, JJ), (thi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          link_title  channel   no_of_views  \\\n",
       "0  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "1  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "2  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "3  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "4  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "\n",
       "  time_uploaded                                            comment upvotes  \\\n",
       "0  Jan 30, 2021  I swear, whoever is playing Plague Inc needs t...     486   \n",
       "1  Jan 30, 2021  I am getting real tired of being part of  majo...     426   \n",
       "2  Jan 30, 2021  This channel's COVID reports are a model of ho...     797   \n",
       "3  Jan 30, 2021  To people who understand science, the developm...     105   \n",
       "4  Jan 30, 2021  So with these new variants this whole thing fe...     205   \n",
       "\n",
       "                                         no_contract  \\\n",
       "0  [I, swear,, whoever, is, playing, Plague, Inc,...   \n",
       "1  [I, am, getting, real, tired, of, being, part,...   \n",
       "2  [This, channel's, COVID, reports, are, a, mode...   \n",
       "3  [To, people, who, understand, science,, the, d...   \n",
       "4  [So, with, these, new, variants, this, whole, ...   \n",
       "\n",
       "                                        comments_str  \\\n",
       "0  I swear, whoever is playing Plague Inc needs t...   \n",
       "1  I am getting real tired of being part of major...   \n",
       "2  This channel's COVID reports are a model of ho...   \n",
       "3  To people who understand science, the developm...   \n",
       "4  So with these new variants this whole thing fe...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [I, swear, ,, whoever, is, playing, Plague, In...   \n",
       "1  [I, am, getting, real, tired, of, being, part,...   \n",
       "2  [This, channel, 's, COVID, reports, are, a, mo...   \n",
       "3  [To, people, who, understand, science, ,, the,...   \n",
       "4  [So, with, these, new, variants, this, whole, ...   \n",
       "\n",
       "                                               lower  \\\n",
       "0  [i, swear, ,, whoever, is, playing, plague, in...   \n",
       "1  [i, am, getting, real, tired, of, being, part,...   \n",
       "2  [this, channel, 's, covid, reports, are, a, mo...   \n",
       "3  [to, people, who, understand, science, ,, the,...   \n",
       "4  [so, with, these, new, variants, this, whole, ...   \n",
       "\n",
       "                                            no_punct  \\\n",
       "0  [i, swear, whoever, is, playing, plague, inc, ...   \n",
       "1  [i, am, getting, real, tired, of, being, part,...   \n",
       "2  [this, channel, 's, covid, reports, are, a, mo...   \n",
       "3  [to, people, who, understand, science, the, de...   \n",
       "4  [so, with, these, new, variants, this, whole, ...   \n",
       "\n",
       "                                        no_stopwords  \\\n",
       "0  [swear, whoever, playing, plague, inc, needs, ...   \n",
       "1  [getting, real, tired, part, major, historical...   \n",
       "2  [channel, 's, covid, reports, model, science, ...   \n",
       "3  [people, understand, science, development, vac...   \n",
       "4  [new, variants, whole, thing, feels, like, fou...   \n",
       "\n",
       "                                            pos_tags  \n",
       "0  [(swear, JJ), (whoever, WP), (playing, VBG), (...  \n",
       "1  [(getting, VBG), (real, JJ), (tired, JJ), (par...  \n",
       "2  [(channel, NN), ('s, POS), (covid, NN), (repor...  \n",
       "3  [(people, NNS), (understand, VBP), (science, N...  \n",
       "4  [(new, JJ), (variants, NNS), (whole, JJ), (thi...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalization - lemmatizing\n",
    "df['pos_tags'] = df['no_stopwords'].apply(nltk.tag.pos_tag)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we convert the speech tags into the appropriate wordnet format\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('r'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "# replace the pos tag with wordnet tags\n",
    "df['wordnet_pos'] = df['pos_tags'].apply(lambda x : [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_title</th>\n",
       "      <th>channel</th>\n",
       "      <th>no_of_views</th>\n",
       "      <th>time_uploaded</th>\n",
       "      <th>comment</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>no_contract</th>\n",
       "      <th>comments_str</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lower</th>\n",
       "      <th>no_punct</th>\n",
       "      <th>no_stopwords</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>wordnet_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>I swear, whoever is playing Plague Inc needs t...</td>\n",
       "      <td>486</td>\n",
       "      <td>[I, swear,, whoever, is, playing, Plague, Inc,...</td>\n",
       "      <td>I swear, whoever is playing Plague Inc needs t...</td>\n",
       "      <td>[I, swear, ,, whoever, is, playing, Plague, In...</td>\n",
       "      <td>[i, swear, ,, whoever, is, playing, plague, in...</td>\n",
       "      <td>[i, swear, whoever, is, playing, plague, inc, ...</td>\n",
       "      <td>[swear, whoever, playing, plague, inc, needs, ...</td>\n",
       "      <td>[(swear, JJ), (whoever, WP), (playing, VBG), (...</td>\n",
       "      <td>[(swear, a), (whoever, n), (playing, v), (plag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>I am getting real tired of being part of  majo...</td>\n",
       "      <td>426</td>\n",
       "      <td>[I, am, getting, real, tired, of, being, part,...</td>\n",
       "      <td>I am getting real tired of being part of major...</td>\n",
       "      <td>[I, am, getting, real, tired, of, being, part,...</td>\n",
       "      <td>[i, am, getting, real, tired, of, being, part,...</td>\n",
       "      <td>[i, am, getting, real, tired, of, being, part,...</td>\n",
       "      <td>[getting, real, tired, part, major, historical...</td>\n",
       "      <td>[(getting, VBG), (real, JJ), (tired, JJ), (par...</td>\n",
       "      <td>[(getting, v), (real, a), (tired, a), (part, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>This channel's COVID reports are a model of ho...</td>\n",
       "      <td>797</td>\n",
       "      <td>[This, channel's, COVID, reports, are, a, mode...</td>\n",
       "      <td>This channel's COVID reports are a model of ho...</td>\n",
       "      <td>[This, channel, 's, COVID, reports, are, a, mo...</td>\n",
       "      <td>[this, channel, 's, covid, reports, are, a, mo...</td>\n",
       "      <td>[this, channel, 's, covid, reports, are, a, mo...</td>\n",
       "      <td>[channel, 's, covid, reports, model, science, ...</td>\n",
       "      <td>[(channel, NN), ('s, POS), (covid, NN), (repor...</td>\n",
       "      <td>[(channel, n), ('s, n), (covid, n), (reports, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>To people who understand science, the developm...</td>\n",
       "      <td>105</td>\n",
       "      <td>[To, people, who, understand, science,, the, d...</td>\n",
       "      <td>To people who understand science, the developm...</td>\n",
       "      <td>[To, people, who, understand, science, ,, the,...</td>\n",
       "      <td>[to, people, who, understand, science, ,, the,...</td>\n",
       "      <td>[to, people, who, understand, science, the, de...</td>\n",
       "      <td>[people, understand, science, development, vac...</td>\n",
       "      <td>[(people, NNS), (understand, VBP), (science, N...</td>\n",
       "      <td>[(people, n), (understand, v), (science, n), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>So with these new variants this whole thing fe...</td>\n",
       "      <td>205</td>\n",
       "      <td>[So, with, these, new, variants, this, whole, ...</td>\n",
       "      <td>So with these new variants this whole thing fe...</td>\n",
       "      <td>[So, with, these, new, variants, this, whole, ...</td>\n",
       "      <td>[so, with, these, new, variants, this, whole, ...</td>\n",
       "      <td>[so, with, these, new, variants, this, whole, ...</td>\n",
       "      <td>[new, variants, whole, thing, feels, like, fou...</td>\n",
       "      <td>[(new, JJ), (variants, NNS), (whole, JJ), (thi...</td>\n",
       "      <td>[(new, a), (variants, n), (whole, a), (thing, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          link_title  channel   no_of_views  \\\n",
       "0  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "1  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "2  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "3  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "4  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "\n",
       "  time_uploaded                                            comment upvotes  \\\n",
       "0  Jan 30, 2021  I swear, whoever is playing Plague Inc needs t...     486   \n",
       "1  Jan 30, 2021  I am getting real tired of being part of  majo...     426   \n",
       "2  Jan 30, 2021  This channel's COVID reports are a model of ho...     797   \n",
       "3  Jan 30, 2021  To people who understand science, the developm...     105   \n",
       "4  Jan 30, 2021  So with these new variants this whole thing fe...     205   \n",
       "\n",
       "                                         no_contract  \\\n",
       "0  [I, swear,, whoever, is, playing, Plague, Inc,...   \n",
       "1  [I, am, getting, real, tired, of, being, part,...   \n",
       "2  [This, channel's, COVID, reports, are, a, mode...   \n",
       "3  [To, people, who, understand, science,, the, d...   \n",
       "4  [So, with, these, new, variants, this, whole, ...   \n",
       "\n",
       "                                        comments_str  \\\n",
       "0  I swear, whoever is playing Plague Inc needs t...   \n",
       "1  I am getting real tired of being part of major...   \n",
       "2  This channel's COVID reports are a model of ho...   \n",
       "3  To people who understand science, the developm...   \n",
       "4  So with these new variants this whole thing fe...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [I, swear, ,, whoever, is, playing, Plague, In...   \n",
       "1  [I, am, getting, real, tired, of, being, part,...   \n",
       "2  [This, channel, 's, COVID, reports, are, a, mo...   \n",
       "3  [To, people, who, understand, science, ,, the,...   \n",
       "4  [So, with, these, new, variants, this, whole, ...   \n",
       "\n",
       "                                               lower  \\\n",
       "0  [i, swear, ,, whoever, is, playing, plague, in...   \n",
       "1  [i, am, getting, real, tired, of, being, part,...   \n",
       "2  [this, channel, 's, covid, reports, are, a, mo...   \n",
       "3  [to, people, who, understand, science, ,, the,...   \n",
       "4  [so, with, these, new, variants, this, whole, ...   \n",
       "\n",
       "                                            no_punct  \\\n",
       "0  [i, swear, whoever, is, playing, plague, inc, ...   \n",
       "1  [i, am, getting, real, tired, of, being, part,...   \n",
       "2  [this, channel, 's, covid, reports, are, a, mo...   \n",
       "3  [to, people, who, understand, science, the, de...   \n",
       "4  [so, with, these, new, variants, this, whole, ...   \n",
       "\n",
       "                                        no_stopwords  \\\n",
       "0  [swear, whoever, playing, plague, inc, needs, ...   \n",
       "1  [getting, real, tired, part, major, historical...   \n",
       "2  [channel, 's, covid, reports, model, science, ...   \n",
       "3  [people, understand, science, development, vac...   \n",
       "4  [new, variants, whole, thing, feels, like, fou...   \n",
       "\n",
       "                                            pos_tags  \\\n",
       "0  [(swear, JJ), (whoever, WP), (playing, VBG), (...   \n",
       "1  [(getting, VBG), (real, JJ), (tired, JJ), (par...   \n",
       "2  [(channel, NN), ('s, POS), (covid, NN), (repor...   \n",
       "3  [(people, NNS), (understand, VBP), (science, N...   \n",
       "4  [(new, JJ), (variants, NNS), (whole, JJ), (thi...   \n",
       "\n",
       "                                         wordnet_pos  \n",
       "0  [(swear, a), (whoever, n), (playing, v), (plag...  \n",
       "1  [(getting, v), (real, a), (tired, a), (part, n...  \n",
       "2  [(channel, n), ('s, n), (covid, n), (reports, ...  \n",
       "3  [(people, n), (understand, v), (science, n), (...  \n",
       "4  [(new, a), (variants, n), (whole, a), (thing, ...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_title</th>\n",
       "      <th>channel</th>\n",
       "      <th>no_of_views</th>\n",
       "      <th>time_uploaded</th>\n",
       "      <th>comment</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>no_contract</th>\n",
       "      <th>comments_str</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lower</th>\n",
       "      <th>no_punct</th>\n",
       "      <th>no_stopwords</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>wordnet_pos</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>I swear, whoever is playing Plague Inc needs t...</td>\n",
       "      <td>486</td>\n",
       "      <td>[I, swear,, whoever, is, playing, Plague, Inc,...</td>\n",
       "      <td>I swear, whoever is playing Plague Inc needs t...</td>\n",
       "      <td>[I, swear, ,, whoever, is, playing, Plague, In...</td>\n",
       "      <td>[i, swear, ,, whoever, is, playing, plague, in...</td>\n",
       "      <td>[i, swear, whoever, is, playing, plague, inc, ...</td>\n",
       "      <td>[swear, whoever, playing, plague, inc, needs, ...</td>\n",
       "      <td>[(swear, JJ), (whoever, WP), (playing, VBG), (...</td>\n",
       "      <td>[(swear, a), (whoever, n), (playing, v), (plag...</td>\n",
       "      <td>[swear, whoever, play, plague, inc, need, stop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>I am getting real tired of being part of  majo...</td>\n",
       "      <td>426</td>\n",
       "      <td>[I, am, getting, real, tired, of, being, part,...</td>\n",
       "      <td>I am getting real tired of being part of major...</td>\n",
       "      <td>[I, am, getting, real, tired, of, being, part,...</td>\n",
       "      <td>[i, am, getting, real, tired, of, being, part,...</td>\n",
       "      <td>[i, am, getting, real, tired, of, being, part,...</td>\n",
       "      <td>[getting, real, tired, part, major, historical...</td>\n",
       "      <td>[(getting, VBG), (real, JJ), (tired, JJ), (par...</td>\n",
       "      <td>[(getting, v), (real, a), (tired, a), (part, n...</td>\n",
       "      <td>[get, real, tired, part, major, historical, ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>This channel's COVID reports are a model of ho...</td>\n",
       "      <td>797</td>\n",
       "      <td>[This, channel's, COVID, reports, are, a, mode...</td>\n",
       "      <td>This channel's COVID reports are a model of ho...</td>\n",
       "      <td>[This, channel, 's, COVID, reports, are, a, mo...</td>\n",
       "      <td>[this, channel, 's, covid, reports, are, a, mo...</td>\n",
       "      <td>[this, channel, 's, covid, reports, are, a, mo...</td>\n",
       "      <td>[channel, 's, covid, reports, model, science, ...</td>\n",
       "      <td>[(channel, NN), ('s, POS), (covid, NN), (repor...</td>\n",
       "      <td>[(channel, n), ('s, n), (covid, n), (reports, ...</td>\n",
       "      <td>[channel, 's, covid, report, model, science, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>To people who understand science, the developm...</td>\n",
       "      <td>105</td>\n",
       "      <td>[To, people, who, understand, science,, the, d...</td>\n",
       "      <td>To people who understand science, the developm...</td>\n",
       "      <td>[To, people, who, understand, science, ,, the,...</td>\n",
       "      <td>[to, people, who, understand, science, ,, the,...</td>\n",
       "      <td>[to, people, who, understand, science, the, de...</td>\n",
       "      <td>[people, understand, science, development, vac...</td>\n",
       "      <td>[(people, NNS), (understand, VBP), (science, N...</td>\n",
       "      <td>[(people, n), (understand, v), (science, n), (...</td>\n",
       "      <td>[people, understand, science, development, vac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We Know More About Those COVID-19 Variants. It...</td>\n",
       "      <td>SciShow</td>\n",
       "      <td>85,566 views</td>\n",
       "      <td>Jan 30, 2021</td>\n",
       "      <td>So with these new variants this whole thing fe...</td>\n",
       "      <td>205</td>\n",
       "      <td>[So, with, these, new, variants, this, whole, ...</td>\n",
       "      <td>So with these new variants this whole thing fe...</td>\n",
       "      <td>[So, with, these, new, variants, this, whole, ...</td>\n",
       "      <td>[so, with, these, new, variants, this, whole, ...</td>\n",
       "      <td>[so, with, these, new, variants, this, whole, ...</td>\n",
       "      <td>[new, variants, whole, thing, feels, like, fou...</td>\n",
       "      <td>[(new, JJ), (variants, NNS), (whole, JJ), (thi...</td>\n",
       "      <td>[(new, a), (variants, n), (whole, a), (thing, ...</td>\n",
       "      <td>[new, variant, whole, thing, feel, like, fough...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          link_title  channel   no_of_views  \\\n",
       "0  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "1  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "2  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "3  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "4  We Know More About Those COVID-19 Variants. It...  SciShow  85,566 views   \n",
       "\n",
       "  time_uploaded                                            comment upvotes  \\\n",
       "0  Jan 30, 2021  I swear, whoever is playing Plague Inc needs t...     486   \n",
       "1  Jan 30, 2021  I am getting real tired of being part of  majo...     426   \n",
       "2  Jan 30, 2021  This channel's COVID reports are a model of ho...     797   \n",
       "3  Jan 30, 2021  To people who understand science, the developm...     105   \n",
       "4  Jan 30, 2021  So with these new variants this whole thing fe...     205   \n",
       "\n",
       "                                         no_contract  \\\n",
       "0  [I, swear,, whoever, is, playing, Plague, Inc,...   \n",
       "1  [I, am, getting, real, tired, of, being, part,...   \n",
       "2  [This, channel's, COVID, reports, are, a, mode...   \n",
       "3  [To, people, who, understand, science,, the, d...   \n",
       "4  [So, with, these, new, variants, this, whole, ...   \n",
       "\n",
       "                                        comments_str  \\\n",
       "0  I swear, whoever is playing Plague Inc needs t...   \n",
       "1  I am getting real tired of being part of major...   \n",
       "2  This channel's COVID reports are a model of ho...   \n",
       "3  To people who understand science, the developm...   \n",
       "4  So with these new variants this whole thing fe...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [I, swear, ,, whoever, is, playing, Plague, In...   \n",
       "1  [I, am, getting, real, tired, of, being, part,...   \n",
       "2  [This, channel, 's, COVID, reports, are, a, mo...   \n",
       "3  [To, people, who, understand, science, ,, the,...   \n",
       "4  [So, with, these, new, variants, this, whole, ...   \n",
       "\n",
       "                                               lower  \\\n",
       "0  [i, swear, ,, whoever, is, playing, plague, in...   \n",
       "1  [i, am, getting, real, tired, of, being, part,...   \n",
       "2  [this, channel, 's, covid, reports, are, a, mo...   \n",
       "3  [to, people, who, understand, science, ,, the,...   \n",
       "4  [so, with, these, new, variants, this, whole, ...   \n",
       "\n",
       "                                            no_punct  \\\n",
       "0  [i, swear, whoever, is, playing, plague, inc, ...   \n",
       "1  [i, am, getting, real, tired, of, being, part,...   \n",
       "2  [this, channel, 's, covid, reports, are, a, mo...   \n",
       "3  [to, people, who, understand, science, the, de...   \n",
       "4  [so, with, these, new, variants, this, whole, ...   \n",
       "\n",
       "                                        no_stopwords  \\\n",
       "0  [swear, whoever, playing, plague, inc, needs, ...   \n",
       "1  [getting, real, tired, part, major, historical...   \n",
       "2  [channel, 's, covid, reports, model, science, ...   \n",
       "3  [people, understand, science, development, vac...   \n",
       "4  [new, variants, whole, thing, feels, like, fou...   \n",
       "\n",
       "                                            pos_tags  \\\n",
       "0  [(swear, JJ), (whoever, WP), (playing, VBG), (...   \n",
       "1  [(getting, VBG), (real, JJ), (tired, JJ), (par...   \n",
       "2  [(channel, NN), ('s, POS), (covid, NN), (repor...   \n",
       "3  [(people, NNS), (understand, VBP), (science, N...   \n",
       "4  [(new, JJ), (variants, NNS), (whole, JJ), (thi...   \n",
       "\n",
       "                                         wordnet_pos  \\\n",
       "0  [(swear, a), (whoever, n), (playing, v), (plag...   \n",
       "1  [(getting, v), (real, a), (tired, a), (part, n...   \n",
       "2  [(channel, n), ('s, n), (covid, n), (reports, ...   \n",
       "3  [(people, n), (understand, v), (science, n), (...   \n",
       "4  [(new, a), (variants, n), (whole, a), (thing, ...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  [swear, whoever, play, plague, inc, need, stop...  \n",
       "1  [get, real, tired, part, major, historical, ev...  \n",
       "2  [channel, 's, covid, report, model, science, r...  \n",
       "3  [people, understand, science, development, vac...  \n",
       "4  [new, variant, whole, thing, feel, like, fough...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatize comments\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['lemmatized'] = df['wordnet_pos'].apply(lambda x: [lemmatizer.lemmatize(word,tag) for (word,tag) in x])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, save cleaned data into csv file\n",
    "df.to_csv('youtube_comments_clean.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
